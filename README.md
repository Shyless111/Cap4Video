<div align="center">

<h2>„ÄêCVPR'2023 üåüHighlightüåü „ÄëCap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval? </h2>


[![Conference](http://img.shields.io/badge/CVPR-2023(Highlight)-f9f107.svg)](https://cvpr.thecvf.com/)
[![Paper](http://img.shields.io/badge/Paper-arxiv.2301.00184-b31b1b.svg)](https://arxiv.org/abs/2301.00184)

[Wenhao Wu](https://whwu95.github.io/)<sup>1,2</sup>, [Haipeng Luo]()<sup>3</sup>, [Bo Fang](https://bofang98.github.io/)<sup>3</sup>,  [Jingdong Wang](https://jingdongwang2017.github.io/)<sup>2</sup>, [Wanli Ouyang](https://wlouyang.github.io/)<sup>4,1</sup>

 
<sup>1</sup>[The University of Sydney](https://www.sydney.edu.au/), <sup>2</sup>[Baidu](https://vis.baidu.com/#/), <sup>3</sup>[UCAS](https://english.ucas.ac.cn/), <sup>4</sup>[Shanghai AI Lab](https://www.shlab.org.cn/)


</div>

***

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/cap4video-what-can-auxiliary-captions-do-for/video-retrieval-on-vatex)](https://paperswithcode.com/sota/video-retrieval-on-vatex?p=cap4video-what-can-auxiliary-captions-do-for)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/cap4video-what-can-auxiliary-captions-do-for/video-retrieval-on-msvd)](https://paperswithcode.com/sota/video-retrieval-on-msvd?p=cap4video-what-can-auxiliary-captions-do-for)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/cap4video-what-can-auxiliary-captions-do-for/video-retrieval-on-msr-vtt-1ka)](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt-1ka?p=cap4video-what-can-auxiliary-captions-do-for)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/cap4video-what-can-auxiliary-captions-do-for/video-retrieval-on-didemo)](https://paperswithcode.com/sota/video-retrieval-on-didemo?p=cap4video-what-can-auxiliary-captions-do-for)

This is the official implementation of the paper [Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?](https://arxiv.org/abs/2301.00184)


<details><summary>üì£ I also have other cross-modal video projects that may interest you ‚ú®. </summary><p>


> [**Revisiting Classifier: Transferring Vision-Language Models for Video Recognition**](https://arxiv.org/abs/2207.01297)<br>
> Accepted by AAAI 2023 | [[Text4Vis Code]](https://github.com/whwu95/Text4Vis)<br>
> Wenhao Wu, Zhun Sun, Wanli Ouyang


> [**Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models**](https://arxiv.org/abs/2301.00182)<br>
> Accepted by CVPR 2023 | [[BIKE Code]](https://github.com/whwu95/BIKE)<br>
> Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, Wanli Ouyang

</p></details>

I am currently traveling and may not be able to open-source the code until May.


## üì£ Updates
- [x] **[Mar 21, 2023]** üòç Our **Cap4Video** has been selected as a **Highlight** paper at CVPR 2023! (Top 2.5% of 9155 submissions).
- [x] **[Feb 28, 2023]** üéâ Our **Cap4Video** has been accepted by **CVPR-2023**.



<a name="bibtex"></a>
## üìå BibTeX & Citation

If you use our code in your research or wish to refer to the results, please star üåü this repo and use the following BibTeX üìë entry.

```
@inproceedings{cap4video,
  title={Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?},
  author={Wu, Wenhao and Luo, Haipeng and Fang, Bo and Wang, Jingdong and Ouyang, Wanli},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}
```
